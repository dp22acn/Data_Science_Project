{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1y0ZoXw_yrx1eppkPDpluqK-MOW5Rq4FD",
      "authorship_tag": "ABX9TyMHYjI7AF613u+epIXf4L8l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dp22acn/Data_Science_Project/blob/main/Python_Code_2_2_Automated__approach.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install spacy"
      ],
      "metadata": {
        "id": "K0bUrTOHK5Mu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyvis"
      ],
      "metadata": {
        "id": "WHs8a3eSHT0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import re\n",
        "import os\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from spacy import displacy\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "bzk8uxHuasNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Add the sentencizer to the pipeline\n",
        "nlp.add_pipe(\"sentencizer\")"
      ],
      "metadata": {
        "id": "8-AbKSK8bCkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First Version OF creating NER MOdel"
      ],
      "metadata": {
        "id": "IYpqPl9iy576"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_books = [b for b in os.scandir(\"/content/drive/MyDrive/mahatxt\") if b.is_file()]"
      ],
      "metadata": {
        "id": "YfJ8V8zFbXVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_books"
      ],
      "metadata": {
        "id": "2cKopEabbs0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "book = all_books[0]\n",
        "book_text = open(book).read()\n",
        "# Increase the max_length limit\n",
        "nlp.max_length = len(book_text) + 1000  # Adding a buffer to be safe\n",
        "book_doc = nlp(book_text)\n"
      ],
      "metadata": {
        "id": "ZdiRIwR0b_KA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "displacy.render(book_doc[0:500], style=\"ent\", jupyter=True)"
      ],
      "metadata": {
        "id": "OBgQMrE2cmDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "character_df = pd.read_csv('/content/Character_Nmaes.csv')"
      ],
      "metadata": {
        "id": "_n1hShFvcrMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_entity_df = []\n",
        "for sent in book_doc.sents:\n",
        "    entity_list = [ent.text for ent in sent.ents]\n",
        "    sent_entity_df.append({\"sentence\":sent, \"entities\":entity_list})\n",
        "\n",
        "sent_entity_df = pd.DataFrame(sent_entity_df)\n",
        "\n",
        "sent_entity_df.head()"
      ],
      "metadata": {
        "id": "WKBQleJUebyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_entity(entity_list, character_df):\n",
        "  return[ent for ent in entity_list\n",
        "         if ent in list(character_df.character)]"
      ],
      "metadata": {
        "id": "9G4HFirrfsU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_entity_df['character_entities'] = sent_entity_df['entities'].apply(lambda x: (filter_entity(x, character_df)))\n",
        "sent_entity_df_fil = sent_entity_df[sent_entity_df['character_entities'].map(len) >0]\n",
        "\n",
        "sent_entity_df_fil.head(10)"
      ],
      "metadata": {
        "id": "REAzl3Lyhwc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.reset_option('^display.', silent=True)\n",
        "sent_entity_df_fil"
      ],
      "metadata": {
        "id": "sQidUAN1ldMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "window_size_ = 7\n",
        "relations = []\n",
        "\n",
        "\n",
        "for i in range(len(sent_entity_df_fil)): # Iterate over the length of the DataFrame\n",
        "  end_i = min(i + window_size_, len(sent_entity_df_fil) -1 )  # Ensure end_i is within bounds\n",
        "  # Extract character entities and flatten the list\n",
        "  char_list = [item for sublist in sent_entity_df_fil.loc[i:end_i, 'character_entities'].tolist() for item in sublist]\n",
        "\n",
        "  char_unq = [char_list[i] for i in range(len(char_list)) if (i == 0) or char_list[i] != char_list[i - 1]]\n",
        "\n",
        "\n",
        "  if len(char_unq) > 1:\n",
        "    for idx, a in enumerate(char_unq[:-1]):\n",
        "      for b in char_unq[idx + 1:]:\n",
        "        relations.append({'Source': a, 'Target': b})\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "g2c8LG1Mokgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "relations_df = pd.DataFrame(relations)\n",
        "relations_df['value'] = 1\n",
        "# Sort only the 'Source' and 'Target' columns before grouping\n",
        "relations_df = relations_df.sort_values(by=['Source', 'Target'])\n",
        "relations_df = relations_df.groupby(['Source', 'Target'], sort=False, as_index=False).sum()\n",
        "relations_df.head()"
      ],
      "metadata": {
        "id": "BFsWaeEMp9WS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.reset_option('^display.', silent=True)\n",
        "relations_df"
      ],
      "metadata": {
        "id": "AHDDgKZQqap7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "G = nx.from_pandas_edgelist(relations_df, source=\"Source\", target=\"Target\", edge_attr=\"value\", create_using=nx.DiGraph())"
      ],
      "metadata": {
        "id": "RKi3uflwsZDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "\n",
        "pos = nx.kamada_kawai_layout(G)\n",
        "\n",
        "nx.draw(G, node_color='green', with_labels=True, edge_cmap=plt.cm.Blues, pos=pos)\n",
        "plt.figure(figsize=(12, 12))\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "cmtVNWG0tyzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyvis.network import Network\n",
        "net = Network(notebook=True, width=\"1000px\", height=\"1000px\", )\n",
        "\n",
        "node_degree = dict(G.degree)\n",
        "nx.set_node_attributes(G, node_degree, 'size')\n",
        "\n",
        "net.from_nx(G)\n",
        "net.show(\"graph.html\")\n",
        "\n",
        "\n",
        "# Exporting  to GEXF\n",
        "nx.write_gexf(G, \"graph_book_1.gexf\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "n9VXEARdud49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converted the model into Def funttion to go through the loop for all books"
      ],
      "metadata": {
        "id": "0d54uzGTO75p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from pyvis.network import Network\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the SpaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Add sentencizer to the pipeline\n",
        "nlp.add_pipe(\"sentencizer\", last=True)\n",
        "\n",
        "# Function to load a book and process it\n",
        "def process_book(book_path, character_df, window_size=10):\n",
        "    with open(book_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        book_text = file.read()\n",
        "\n",
        "    # Adjust spaCy max_length\n",
        "    nlp.max_length = len(book_text) + 1000\n",
        "\n",
        "    # Process the book text\n",
        "    book_doc = nlp(book_text)\n",
        "\n",
        "    # Extract sentences and entities\n",
        "    sent_entity_df = []\n",
        "    for sent in book_doc.sents:\n",
        "        entity_list = [ent.text for ent in sent.ents]\n",
        "        sent_entity_df.append({\"sentence\": sent, \"entities\": entity_list})\n",
        "\n",
        "    sent_entity_df = pd.DataFrame(sent_entity_df)\n",
        "\n",
        "    # Filter entities based on the character list\n",
        "    sent_entity_df['character_entities'] = sent_entity_df['entities'].apply(\n",
        "        lambda x: [ent for ent in x if ent in list(character_df.character)]\n",
        "    )\n",
        "    sent_entity_df_fil = sent_entity_df[sent_entity_df['character_entities'].map(len) > 0]\n",
        "\n",
        "    # Extract relationships\n",
        "    relations = []\n",
        "    for i in range(len(sent_entity_df_fil)):\n",
        "        end_i = min(i + window_size, len(sent_entity_df_fil) - 1)\n",
        "        char_list = [\n",
        "            item for sublist in sent_entity_df_fil.loc[i:end_i, 'character_entities'].tolist() for item in sublist\n",
        "        ]\n",
        "        char_unq = [char_list[i] for i in range(len(char_list)) if (i == 0) or char_list[i] != char_list[i - 1]]\n",
        "        if len(char_unq) > 1:\n",
        "            for idx, a in enumerate(char_unq[:-1]):\n",
        "                for b in char_unq[idx + 1:]:\n",
        "                    relations.append({'Source': a, 'Target': b})\n",
        "\n",
        "    # Create a DataFrame for relationships\n",
        "    relations_df = pd.DataFrame(relations)\n",
        "    if not relations_df.empty:\n",
        "        relations_df = pd.DataFrame(np.sort(relations_df.values, axis=1), columns=relations_df.columns)\n",
        "        relations_df[\"value\"] = 1\n",
        "        relations_df = relations_df.groupby([\"Source\", \"Target\"], sort=False, as_index=False).sum()\n",
        "    return relations_df\n",
        "\n",
        "# Function to create and save the graph\n",
        "def create_and_save_graph(relations_df, book_number):\n",
        "    # Create the network graph\n",
        "    G = nx.from_pandas_edgelist(relations_df, source=\"Source\", target=\"Target\", edge_attr=\"value\", create_using=nx.DiGraph())\n",
        "\n",
        "    # Visualize using matplotlib\n",
        "    pos = nx.kamada_kawai_layout(G)\n",
        "    plt.figure(figsize=(12, 12))\n",
        "    nx.draw(G, pos, node_color='green', with_labels=True, edge_cmap=plt.cm.Blues)\n",
        "    plt.show()\n",
        "\n",
        "    # Visualize using pyvis\n",
        "    net = Network(notebook=True, width=\"1000px\", height=\"1000px\")\n",
        "    node_degree = dict(G.degree)\n",
        "    nx.set_node_attributes(G, node_degree, 'size')\n",
        "    net.from_nx(G)\n",
        "    net.show(f\"graph_{book_number}.html\")\n",
        "\n",
        "    # Save as GEXF file\n",
        "    nx.write_gexf(G, f\"graph_book_{book_number}.gexf\")\n",
        "    print(f\"GEXF file saved for Book {book_number} as graph_book_{book_number}.gexf\")\n",
        "\n",
        "# Main processing function for all books\n",
        "def process_all_books(folder_path, character_csv, window_size=10):\n",
        "    character_df = pd.read_csv(character_csv)\n",
        "    all_books = [b for b in os.scandir(folder_path) if b.is_file() and b.name.endswith(\".txt\")]\n",
        "\n",
        "    for idx, book in enumerate(all_books, start=1):\n",
        "        print(f\"Processing Book {idx}: {book.name}\")\n",
        "        relations_df = process_book(book.path, character_df, window_size)\n",
        "        if not relations_df.empty:\n",
        "            create_and_save_graph(relations_df, idx)\n",
        "        else:\n",
        "            print(f\"No relationships found in Book {idx}: {book.name}\")\n",
        "\n",
        "# Run the process for all books in the folder\n",
        "books_folder = \"/content/drive/MyDrive/data\"\n",
        "character_csv = \"/content/filtered_characters.csv\"\n",
        "process_all_books(books_folder, character_csv)\n"
      ],
      "metadata": {
        "id": "wjgaTTeweUMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Version 2"
      ],
      "metadata": {
        "id": "Lz03asnIO_7E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import spacy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "from pyvis.network import Network\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the SpaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Add sentencizer to the pipeline\n",
        "nlp.add_pipe(\"sentencizer\", last=True)\n",
        "\n",
        "# Function to load a book and process it\n",
        "def process_book(book_path, character_df, window_size=5):\n",
        "    with open(book_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        book_text = file.read()\n",
        "\n",
        "    # Adjust spaCy max_length\n",
        "    nlp.max_length = len(book_text) + 1000\n",
        "\n",
        "    # Process the book text\n",
        "    book_doc = nlp(book_text)\n",
        "\n",
        "    # Extract sentences and entities\n",
        "    sent_entity_df = []\n",
        "    for sent in book_doc.sents:\n",
        "        entity_list = [ent.text for ent in sent.ents]\n",
        "        sent_entity_df.append({\"sentence\": sent, \"entities\": entity_list})\n",
        "\n",
        "    sent_entity_df = pd.DataFrame(sent_entity_df)\n",
        "\n",
        "    # Filter entities based on the character list\n",
        "    sent_entity_df['character_entities'] = sent_entity_df['entities'].apply(\n",
        "        lambda x: [ent for ent in x if ent in list(character_df.character)]\n",
        "    )\n",
        "    sent_entity_df_fil = sent_entity_df[sent_entity_df['character_entities'].map(len) > 0]\n",
        "\n",
        "    # Extract relationships\n",
        "    relations = []\n",
        "    for i in range(len(sent_entity_df_fil)):\n",
        "        end_i = min(i + window_size, len(sent_entity_df_fil) - 1)\n",
        "        char_list = [\n",
        "            item for sublist in sent_entity_df_fil.loc[i:end_i, 'character_entities'].tolist() for item in sublist\n",
        "        ]\n",
        "        char_unq = [char_list[i] for i in range(len(char_list)) if (i == 0) or char_list[i] != char_list[i - 1]]\n",
        "        if len(char_unq) > 1:\n",
        "            for idx, a in enumerate(char_unq[:-1]):\n",
        "                for b in char_unq[idx + 1:]:\n",
        "                    if a != b:  # Remove self-loops\n",
        "                        relations.append({'Source': a, 'Target': b})\n",
        "\n",
        "    # Create a DataFrame for relationships\n",
        "    relations_df = pd.DataFrame(relations)\n",
        "    if not relations_df.empty:\n",
        "        relations_df[\"value\"] = 1\n",
        "        relations_df = relations_df.groupby([\"Source\", \"Target\"], sort=False, as_index=False).sum()\n",
        "    return relations_df\n",
        "\n",
        "# Function to create and save the graph\n",
        "def create_and_save_graph(relations_df, book_number):\n",
        "    # Create the directed graph\n",
        "    G = nx.from_pandas_edgelist(\n",
        "        relations_df, source=\"Source\", target=\"Target\", edge_attr=\"value\", create_using=nx.DiGraph()\n",
        "    )\n",
        "\n",
        "    # Visualize using matplotlib\n",
        "    pos = nx.kamada_kawai_layout(G)\n",
        "    plt.figure(figsize=(12, 12))\n",
        "    nx.draw(G, pos, node_color='green', with_labels=True, edge_cmap=plt.cm.Blues)\n",
        "    plt.show()\n",
        "\n",
        "    # Visualize using pyvis\n",
        "    net = Network(notebook=True, width=\"1000px\", height=\"1000px\", directed=True)\n",
        "    node_degree = dict(G.degree)\n",
        "    nx.set_node_attributes(G, node_degree, 'size')\n",
        "    net.from_nx(G)\n",
        "    net.show(f\"graph_{book_number}.html\")\n",
        "\n",
        "    # Save as GEXF file\n",
        "    nx.write_gexf(G, f\"graph_book_{book_number}.gexf\")\n",
        "    print(f\"GEXF file saved for Book {book_number} as graph_book_{book_number}.gexf\")\n",
        "\n",
        "# Main processing function for all books\n",
        "def process_all_books(folder_path, character_csv, window_size=10):\n",
        "    character_df = pd.read_csv(character_csv)\n",
        "    all_books = [b for b in os.scandir(folder_path) if b.is_file() and b.name.endswith(\".txt\")]\n",
        "\n",
        "    for idx, book in enumerate(all_books, start=1):\n",
        "        print(f\"Processing Book {idx}: {book.name}\")\n",
        "        relations_df = process_book(book.path, character_df, window_size)\n",
        "        if not relations_df.empty:\n",
        "            create_and_save_graph(relations_df, idx)\n",
        "        else:\n",
        "            print(f\"No relationships found in Book {idx}: {book.name}\")\n",
        "\n",
        "# Run the process for all books in the folder\n",
        "books_folder = \"/content/drive/MyDrive/mahatxt\"\n",
        "character_csv = \"/content/Character_Nmaes.csv\"\n",
        "process_all_books(books_folder, character_csv)\n"
      ],
      "metadata": {
        "id": "mTwcl3Rngjdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final Version"
      ],
      "metadata": {
        "id": "wLr7Ez0BPHBo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import spacy\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "from pyvis.network import Network\n",
        "\n",
        "# Load the SpaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Add sentencizer to the pipeline\n",
        "nlp.add_pipe(\"sentencizer\", last=True)\n",
        "\n",
        "# Function to load a book and process it\n",
        "def process_book(book_path, character_df, alt_name_map, window_size=5):\n",
        "    with open(book_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        book_text = file.read()\n",
        "\n",
        "    # Adjust spaCy max_length\n",
        "    nlp.max_length = len(book_text) + 1000\n",
        "\n",
        "    # Process the book text\n",
        "    book_doc = nlp(book_text)\n",
        "\n",
        "    # Extract sentences and entities\n",
        "    sent_entity_df = []\n",
        "    for sent in book_doc.sents:\n",
        "        entity_list = [ent.text for ent in sent.ents]\n",
        "        # Normalize entities using the alt_name_map\n",
        "        normalized_entities = [alt_name_map.get(ent, ent) for ent in entity_list]\n",
        "        sent_entity_df.append({\"sentence\": sent, \"entities\": normalized_entities})\n",
        "\n",
        "    sent_entity_df = pd.DataFrame(sent_entity_df)\n",
        "\n",
        "    # Filter entities based on the character list\n",
        "    sent_entity_df['character_entities'] = sent_entity_df['entities'].apply(\n",
        "        lambda x: [ent for ent in x if ent in list(character_df.character)]\n",
        "    )\n",
        "    sent_entity_df_fil = sent_entity_df[sent_entity_df['character_entities'].map(len) > 0]\n",
        "\n",
        "    # Extract relationships\n",
        "    relations = []\n",
        "    for i in range(len(sent_entity_df_fil)):\n",
        "        end_i = min(i + window_size, len(sent_entity_df_fil) - 1)\n",
        "        char_list = [\n",
        "            item for sublist in sent_entity_df_fil.loc[i:end_i, 'character_entities'].tolist() for item in sublist\n",
        "        ]\n",
        "        char_unq = [char_list[i] for i in range(len(char_list)) if (i == 0) or char_list[i] != char_list[i - 1]]\n",
        "        if len(char_unq) > 1:\n",
        "            for idx, a in enumerate(char_unq[:-1]):\n",
        "                for b in char_unq[idx + 1:]:\n",
        "                    if a != b:  # Remove self-loops\n",
        "                        relations.append({'Source': a, 'Target': b})\n",
        "\n",
        "    # Create a DataFrame for relationships\n",
        "    relations_df = pd.DataFrame(relations)\n",
        "    if not relations_df.empty:\n",
        "        relations_df[\"value\"] = 1\n",
        "        relations_df = relations_df.groupby([\"Source\", \"Target\"], sort=False, as_index=False).sum()\n",
        "    return relations_df\n",
        "\n",
        "# Function to create and save the graph\n",
        "def create_and_save_graph(relations_df, book_number):\n",
        "    # Create the directed graph\n",
        "    G = nx.from_pandas_edgelist(\n",
        "        relations_df, source=\"Source\", target=\"Target\", edge_attr=\"value\", create_using=nx.DiGraph()\n",
        "    )\n",
        "\n",
        "    # Create a dynamic graph for the chapter (book)\n",
        "    net = Network(notebook=True, width=\"1000px\", height=\"1000px\", directed=True)\n",
        "    node_degree = dict(G.degree)\n",
        "    nx.set_node_attributes(G, node_degree, 'size')\n",
        "    net.from_nx(G)\n",
        "\n",
        "    # Save as GEXF file\n",
        "    nx.write_gexf(G, f\"graph_book_{book_number}.gexf\")\n",
        "    print(f\"GEXF file saved for Book {book_number} as graph_book_{book_number}.gexf\")\n",
        "\n",
        "# Main processing function for all books\n",
        "def process_all_books(folder_path, character_csv, window_size=10):\n",
        "    # Read the character data and create a mapping for alternative names\n",
        "    character_df = pd.read_csv(character_csv)\n",
        "    alt_name_map = {}\n",
        "\n",
        "    # Populate the alternative name map\n",
        "    for _, row in character_df.iterrows():\n",
        "        primary_name = row['character']\n",
        "        if pd.notna(row['alternative_names']):\n",
        "            alternatives = [alt.strip() for alt in row['alternative_names'].split(',')]\n",
        "            for alt in alternatives:\n",
        "                alt_name_map[alt] = primary_name\n",
        "\n",
        "    all_books = [b for b in os.scandir(folder_path) if b.is_file() and b.name.endswith(\".txt\")]\n",
        "\n",
        "    for idx, book in enumerate(all_books, start=1):\n",
        "        print(f\"Processing Book {idx}: {book.name}\")\n",
        "        relations_df = process_book(book.path, character_df, alt_name_map, window_size)\n",
        "        if not relations_df.empty:\n",
        "            create_and_save_graph(relations_df, idx)\n",
        "        else:\n",
        "            print(f\"No relationships found in Book {idx}: {book.name}\")\n",
        "\n",
        "# Run the process for all books in the folder\n",
        "books_folder = \"/content/drive/MyDrive/mahatxt\"\n",
        "character_csv = \"/content/drive/MyDrive/character_files/processed_names_from_books.csv\"\n",
        "process_all_books(books_folder, character_csv)\n"
      ],
      "metadata": {
        "id": "Ws9UzHEIOIvd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}